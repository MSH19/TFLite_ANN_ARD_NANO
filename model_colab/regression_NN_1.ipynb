{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HCB1tc0kH-sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e6af5f-c26a-411c-8602-3570c9e3e3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "10/10 - 1s - loss: 0.5269 - mae: 0.6605 - val_loss: 0.3918 - val_mae: 0.5575 - 1s/epoch - 107ms/step\n",
            "Epoch 2/50\n",
            "10/10 - 0s - loss: 0.3646 - mae: 0.5380 - val_loss: 0.3052 - val_mae: 0.4809 - 211ms/epoch - 21ms/step\n",
            "Epoch 3/50\n",
            "10/10 - 0s - loss: 0.2938 - mae: 0.4770 - val_loss: 0.2547 - val_mae: 0.4371 - 101ms/epoch - 10ms/step\n",
            "Epoch 4/50\n",
            "10/10 - 0s - loss: 0.2483 - mae: 0.4373 - val_loss: 0.2210 - val_mae: 0.4082 - 79ms/epoch - 8ms/step\n",
            "Epoch 5/50\n",
            "10/10 - 0s - loss: 0.2156 - mae: 0.4072 - val_loss: 0.1946 - val_mae: 0.3846 - 76ms/epoch - 8ms/step\n",
            "Epoch 6/50\n",
            "10/10 - 0s - loss: 0.1896 - mae: 0.3823 - val_loss: 0.1748 - val_mae: 0.3659 - 88ms/epoch - 9ms/step\n",
            "Epoch 7/50\n",
            "10/10 - 0s - loss: 0.1693 - mae: 0.3624 - val_loss: 0.1596 - val_mae: 0.3505 - 79ms/epoch - 8ms/step\n",
            "Epoch 8/50\n",
            "10/10 - 0s - loss: 0.1534 - mae: 0.3454 - val_loss: 0.1477 - val_mae: 0.3378 - 83ms/epoch - 8ms/step\n",
            "Epoch 9/50\n",
            "10/10 - 0s - loss: 0.1407 - mae: 0.3310 - val_loss: 0.1387 - val_mae: 0.3281 - 82ms/epoch - 8ms/step\n",
            "Epoch 10/50\n",
            "10/10 - 0s - loss: 0.1309 - mae: 0.3194 - val_loss: 0.1317 - val_mae: 0.3203 - 85ms/epoch - 9ms/step\n",
            "Epoch 11/50\n",
            "10/10 - 0s - loss: 0.1228 - mae: 0.3091 - val_loss: 0.1263 - val_mae: 0.3136 - 91ms/epoch - 9ms/step\n",
            "Epoch 12/50\n",
            "10/10 - 0s - loss: 0.1160 - mae: 0.3000 - val_loss: 0.1225 - val_mae: 0.3081 - 90ms/epoch - 9ms/step\n",
            "Epoch 13/50\n",
            "10/10 - 0s - loss: 0.1112 - mae: 0.2924 - val_loss: 0.1190 - val_mae: 0.3033 - 94ms/epoch - 9ms/step\n",
            "Epoch 14/50\n",
            "10/10 - 0s - loss: 0.1068 - mae: 0.2858 - val_loss: 0.1163 - val_mae: 0.2990 - 88ms/epoch - 9ms/step\n",
            "Epoch 15/50\n",
            "10/10 - 0s - loss: 0.1026 - mae: 0.2790 - val_loss: 0.1140 - val_mae: 0.2956 - 55ms/epoch - 5ms/step\n",
            "Epoch 16/50\n",
            "10/10 - 0s - loss: 0.0997 - mae: 0.2746 - val_loss: 0.1121 - val_mae: 0.2918 - 61ms/epoch - 6ms/step\n",
            "Epoch 17/50\n",
            "10/10 - 0s - loss: 0.0968 - mae: 0.2695 - val_loss: 0.1098 - val_mae: 0.2875 - 46ms/epoch - 5ms/step\n",
            "Epoch 18/50\n",
            "10/10 - 0s - loss: 0.0934 - mae: 0.2642 - val_loss: 0.1072 - val_mae: 0.2821 - 48ms/epoch - 5ms/step\n",
            "Epoch 19/50\n",
            "10/10 - 0s - loss: 0.0902 - mae: 0.2582 - val_loss: 0.1053 - val_mae: 0.2785 - 62ms/epoch - 6ms/step\n",
            "Epoch 20/50\n",
            "10/10 - 0s - loss: 0.0881 - mae: 0.2554 - val_loss: 0.1041 - val_mae: 0.2754 - 66ms/epoch - 7ms/step\n",
            "Epoch 21/50\n",
            "10/10 - 0s - loss: 0.0864 - mae: 0.2515 - val_loss: 0.1031 - val_mae: 0.2725 - 59ms/epoch - 6ms/step\n",
            "Epoch 22/50\n",
            "10/10 - 0s - loss: 0.0850 - mae: 0.2480 - val_loss: 0.1016 - val_mae: 0.2706 - 49ms/epoch - 5ms/step\n",
            "Epoch 23/50\n",
            "10/10 - 0s - loss: 0.0837 - mae: 0.2463 - val_loss: 0.1005 - val_mae: 0.2682 - 47ms/epoch - 5ms/step\n",
            "Epoch 24/50\n",
            "10/10 - 0s - loss: 0.0823 - mae: 0.2438 - val_loss: 0.0992 - val_mae: 0.2655 - 64ms/epoch - 6ms/step\n",
            "Epoch 25/50\n",
            "10/10 - 0s - loss: 0.0810 - mae: 0.2414 - val_loss: 0.0981 - val_mae: 0.2629 - 60ms/epoch - 6ms/step\n",
            "Epoch 26/50\n",
            "10/10 - 0s - loss: 0.0797 - mae: 0.2390 - val_loss: 0.0969 - val_mae: 0.2608 - 65ms/epoch - 7ms/step\n",
            "Epoch 27/50\n",
            "10/10 - 0s - loss: 0.0785 - mae: 0.2362 - val_loss: 0.0959 - val_mae: 0.2586 - 71ms/epoch - 7ms/step\n",
            "Epoch 28/50\n",
            "10/10 - 0s - loss: 0.0773 - mae: 0.2340 - val_loss: 0.0943 - val_mae: 0.2572 - 61ms/epoch - 6ms/step\n",
            "Epoch 29/50\n",
            "10/10 - 0s - loss: 0.0761 - mae: 0.2333 - val_loss: 0.0932 - val_mae: 0.2552 - 66ms/epoch - 7ms/step\n",
            "Epoch 30/50\n",
            "10/10 - 0s - loss: 0.0751 - mae: 0.2310 - val_loss: 0.0922 - val_mae: 0.2536 - 48ms/epoch - 5ms/step\n",
            "Epoch 31/50\n",
            "10/10 - 0s - loss: 0.0739 - mae: 0.2286 - val_loss: 0.0909 - val_mae: 0.2518 - 48ms/epoch - 5ms/step\n",
            "Epoch 32/50\n",
            "10/10 - 0s - loss: 0.0730 - mae: 0.2273 - val_loss: 0.0899 - val_mae: 0.2501 - 60ms/epoch - 6ms/step\n",
            "Epoch 33/50\n",
            "10/10 - 0s - loss: 0.0719 - mae: 0.2255 - val_loss: 0.0888 - val_mae: 0.2487 - 59ms/epoch - 6ms/step\n",
            "Epoch 34/50\n",
            "10/10 - 0s - loss: 0.0709 - mae: 0.2235 - val_loss: 0.0868 - val_mae: 0.2457 - 63ms/epoch - 6ms/step\n",
            "Epoch 35/50\n",
            "10/10 - 0s - loss: 0.0689 - mae: 0.2202 - val_loss: 0.0844 - val_mae: 0.2421 - 52ms/epoch - 5ms/step\n",
            "Epoch 36/50\n",
            "10/10 - 0s - loss: 0.0674 - mae: 0.2172 - val_loss: 0.0823 - val_mae: 0.2391 - 48ms/epoch - 5ms/step\n",
            "Epoch 37/50\n",
            "10/10 - 0s - loss: 0.0659 - mae: 0.2146 - val_loss: 0.0808 - val_mae: 0.2370 - 46ms/epoch - 5ms/step\n",
            "Epoch 38/50\n",
            "10/10 - 0s - loss: 0.0647 - mae: 0.2129 - val_loss: 0.0796 - val_mae: 0.2344 - 48ms/epoch - 5ms/step\n",
            "Epoch 39/50\n",
            "10/10 - 0s - loss: 0.0638 - mae: 0.2110 - val_loss: 0.0783 - val_mae: 0.2325 - 60ms/epoch - 6ms/step\n",
            "Epoch 40/50\n",
            "10/10 - 0s - loss: 0.0625 - mae: 0.2089 - val_loss: 0.0777 - val_mae: 0.2302 - 50ms/epoch - 5ms/step\n",
            "Epoch 41/50\n",
            "10/10 - 0s - loss: 0.0616 - mae: 0.2059 - val_loss: 0.0767 - val_mae: 0.2281 - 53ms/epoch - 5ms/step\n",
            "Epoch 42/50\n",
            "10/10 - 0s - loss: 0.0606 - mae: 0.2041 - val_loss: 0.0759 - val_mae: 0.2261 - 45ms/epoch - 5ms/step\n",
            "Epoch 43/50\n",
            "10/10 - 0s - loss: 0.0596 - mae: 0.2013 - val_loss: 0.0746 - val_mae: 0.2241 - 78ms/epoch - 8ms/step\n",
            "Epoch 44/50\n",
            "10/10 - 0s - loss: 0.0587 - mae: 0.1993 - val_loss: 0.0737 - val_mae: 0.2222 - 58ms/epoch - 6ms/step\n",
            "Epoch 45/50\n",
            "10/10 - 0s - loss: 0.0578 - mae: 0.1977 - val_loss: 0.0724 - val_mae: 0.2205 - 66ms/epoch - 7ms/step\n",
            "Epoch 46/50\n",
            "10/10 - 0s - loss: 0.0570 - mae: 0.1964 - val_loss: 0.0716 - val_mae: 0.2185 - 59ms/epoch - 6ms/step\n",
            "Epoch 47/50\n",
            "10/10 - 0s - loss: 0.0560 - mae: 0.1941 - val_loss: 0.0704 - val_mae: 0.2168 - 46ms/epoch - 5ms/step\n",
            "Epoch 48/50\n",
            "10/10 - 0s - loss: 0.0551 - mae: 0.1927 - val_loss: 0.0698 - val_mae: 0.2149 - 52ms/epoch - 5ms/step\n",
            "Epoch 49/50\n",
            "10/10 - 0s - loss: 0.0543 - mae: 0.1902 - val_loss: 0.0686 - val_mae: 0.2129 - 61ms/epoch - 6ms/step\n",
            "Epoch 50/50\n",
            "10/10 - 0s - loss: 0.0534 - mae: 0.1882 - val_loss: 0.0672 - val_mae: 0.2115 - 59ms/epoch - 6ms/step\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0642 - mae: 0.2063\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "[[-0.55176514]\n",
            " [ 0.48662168]\n",
            " [ 0.715858  ]\n",
            " [-0.69092107]\n",
            " [-0.6418241 ]\n",
            " [-0.03303707]\n",
            " [-0.3987705 ]\n",
            " [-0.6785883 ]\n",
            " [ 0.39994788]\n",
            " [ 0.65831554]\n",
            " [ 0.6909555 ]\n",
            " [-0.00384969]\n",
            " [ 0.71838164]\n",
            " [-0.64043987]\n",
            " [ 0.6940352 ]\n",
            " [ 0.72621024]\n",
            " [ 0.52048796]\n",
            " [-0.698406  ]\n",
            " [ 0.55110246]\n",
            " [-0.11996794]\n",
            " [ 0.7092066 ]\n",
            " [ 0.72126293]\n",
            " [ 0.7265672 ]\n",
            " [ 0.7200161 ]\n",
            " [ 0.728286  ]\n",
            " [ 0.70216936]\n",
            " [ 0.67418295]\n",
            " [ 0.6920856 ]\n",
            " [-0.6855818 ]\n",
            " [ 0.6866224 ]\n",
            " [ 0.6623267 ]\n",
            " [ 0.678056  ]\n",
            " [ 0.69871867]\n",
            " [-0.6797495 ]\n",
            " [ 0.68008566]\n",
            " [-0.68067527]\n",
            " [-0.6893004 ]\n",
            " [-0.20763588]\n",
            " [ 0.32450625]\n",
            " [ 0.428572  ]\n",
            " [-0.6864817 ]\n",
            " [ 0.5327977 ]\n",
            " [ 0.715289  ]\n",
            " [ 0.6993017 ]\n",
            " [ 0.6783551 ]\n",
            " [-0.43032873]\n",
            " [ 0.70886886]\n",
            " [-0.23505467]\n",
            " [ 0.7259941 ]\n",
            " [-0.09353453]\n",
            " [-0.6792606 ]\n",
            " [ 0.6908486 ]\n",
            " [-0.37706268]\n",
            " [-0.64946836]\n",
            " [ 0.6911963 ]\n",
            " [-0.6963887 ]\n",
            " [ 0.6741933 ]\n",
            " [-0.67801726]\n",
            " [ 0.2340546 ]\n",
            " [-0.68450487]\n",
            " [ 0.5486388 ]\n",
            " [-0.31465888]\n",
            " [ 0.72505856]\n",
            " [ 0.68504524]\n",
            " [ 0.72742736]\n",
            " [ 0.68281937]\n",
            " [ 0.6818663 ]\n",
            " [ 0.51322985]\n",
            " [-0.6844462 ]\n",
            " [-0.21009731]\n",
            " [ 0.68407094]\n",
            " [ 0.72677803]\n",
            " [ 0.42723948]\n",
            " [ 0.70144445]\n",
            " [-0.67752445]\n",
            " [ 0.7188386 ]\n",
            " [ 0.70339096]\n",
            " [ 0.1940695 ]\n",
            " [ 0.6819511 ]\n",
            " [ 0.7160845 ]\n",
            " [-0.68482864]\n",
            " [ 0.70780706]\n",
            " [-0.69737446]\n",
            " [-0.5833567 ]\n",
            " [ 0.721366  ]\n",
            " [ 0.47105247]\n",
            " [ 0.6382525 ]\n",
            " [-0.68520844]\n",
            " [ 0.69281805]\n",
            " [-0.69734156]\n",
            " [-0.6973076 ]\n",
            " [-0.6823493 ]\n",
            " [ 0.69147545]\n",
            " [-0.679865  ]\n",
            " [-0.62726593]\n",
            " [-0.6852486 ]\n",
            " [-0.6391998 ]\n",
            " [ 0.7206465 ]\n",
            " [ 0.7178005 ]\n",
            " [-0.69046557]\n",
            " [ 0.29551578]\n",
            " [ 0.01368243]\n",
            " [ 0.7150842 ]\n",
            " [-0.6889777 ]\n",
            " [ 0.71713173]\n",
            " [ 0.728307  ]\n",
            " [-0.6890545 ]\n",
            " [ 0.7273385 ]\n",
            " [-0.69779265]\n",
            " [-0.6798643 ]\n",
            " [-0.683226  ]\n",
            " [-0.6495367 ]\n",
            " [ 0.71469367]\n",
            " [-0.6843535 ]\n",
            " [ 0.7170653 ]\n",
            " [-0.69719684]\n",
            " [-0.6871835 ]\n",
            " [-0.67435133]\n",
            " [ 0.24662498]\n",
            " [ 0.6871512 ]\n",
            " [ 0.46298254]\n",
            " [-0.6981933 ]\n",
            " [ 0.7035614 ]\n",
            " [ 0.67961437]\n",
            " [ 0.7123008 ]\n",
            " [ 0.6260035 ]\n",
            " [-0.66533744]\n",
            " [-0.677673  ]\n",
            " [-0.6780406 ]\n",
            " [-0.69690454]\n",
            " [-0.6965239 ]\n",
            " [-0.58436346]\n",
            " [ 0.3517085 ]\n",
            " [-0.6784797 ]\n",
            " [-0.67991984]\n",
            " [ 0.72500795]\n",
            " [-0.69761825]\n",
            " [ 0.6346277 ]\n",
            " [-0.677691  ]\n",
            " [-0.6925918 ]\n",
            " [-0.68470716]\n",
            " [ 0.7082126 ]\n",
            " [ 0.01886344]\n",
            " [-0.24661684]\n",
            " [ 0.70490587]\n",
            " [ 0.63130844]\n",
            " [ 0.69112885]\n",
            " [-0.40834332]\n",
            " [-0.67685556]\n",
            " [-0.6990222 ]\n",
            " [-0.3816889 ]\n",
            " [-0.6851493 ]\n",
            " [ 0.67777663]\n",
            " [ 0.393923  ]\n",
            " [-0.577639  ]\n",
            " [-0.67633927]\n",
            " [ 0.72078013]\n",
            " [ 0.6741884 ]\n",
            " [-0.41011506]\n",
            " [ 0.56704545]\n",
            " [-0.45439065]\n",
            " [ 0.67522204]\n",
            " [-0.6834589 ]\n",
            " [ 0.7097645 ]\n",
            " [-0.69127405]\n",
            " [-0.06747687]\n",
            " [ 0.7202352 ]\n",
            " [-0.6800834 ]\n",
            " [ 0.6906284 ]\n",
            " [ 0.72909784]\n",
            " [ 0.3711699 ]\n",
            " [ 0.69305825]\n",
            " [-0.6908796 ]\n",
            " [ 0.67981553]\n",
            " [-0.5803166 ]\n",
            " [-0.6926701 ]\n",
            " [-0.69687307]\n",
            " [ 0.68189335]\n",
            " [ 0.7244635 ]\n",
            " [ 0.02762306]\n",
            " [ 0.71723735]\n",
            " [ 0.70161784]\n",
            " [ 0.00307474]\n",
            " [-0.14464831]\n",
            " [-0.6959888 ]\n",
            " [ 0.6843263 ]\n",
            " [-0.6971673 ]\n",
            " [-0.60709697]\n",
            " [ 0.6862285 ]\n",
            " [-0.68080854]\n",
            " [-0.68152666]\n",
            " [ 0.7175802 ]\n",
            " [-0.50158894]\n",
            " [-0.13472402]\n",
            " [-0.69029546]\n",
            " [-0.3833235 ]\n",
            " [ 0.56034315]\n",
            " [-0.6763307 ]\n",
            " [-0.1425994 ]\n",
            " [-0.45360172]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "# generate artificial sine signal data\n",
        "# x_values are the processs data (float32)\n",
        "x_values = np.random.uniform(low=0, high=2*math.pi, size= 1000).astype(np.float32)\n",
        "np.random.shuffle(x_values)\n",
        "# y_values are the data labels (float32)\n",
        "y_values = np.sin(x_values.astype(np.float32))\n",
        "\n",
        "# split the data into training, validation, and testing\n",
        "samples = 1000\n",
        "train_split = int(0.6*samples)\n",
        "test_split = int(0.2*samples + train_split)\n",
        "x_train, x_valid, x_test = np.split (x_values, [train_split, test_split])\n",
        "y_train, y_valid, y_test = np.split (y_values, [train_split, test_split])\n",
        "\n",
        "# normalize the data by subtrating the mean and dividing by the standard\n",
        "# deviation (-1 and +1)\n",
        "mean = x_train.mean(axis=0)\n",
        "std = x_train.std(axis=0)\n",
        "x_train -= mean\n",
        "x_train /= std\n",
        "x_valid -= mean\n",
        "x_valid /= std\n",
        "x_test -= mean\n",
        "x_test /= std\n",
        "\n",
        "# export the test data to a text file\n",
        "np.savetxt('x_test_normalized.txt', x_test, fmt='%.6f', delimiter=',')\n",
        "np.savetxt('y_test.txt', y_test, fmt='%.6f', delimiter=',')\n",
        "\n",
        "# define the model\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# add layers to the model (input 16, middle 16, last 1 (outputting a number))\n",
        "model.add(tf.keras.layers.Dense(16, activation= 'relu', input_shape=(1, )))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "# compile the model (optimizer function is stochastic gradient descent, and loss\n",
        "# is MSE)\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['mae'])\n",
        "# optimizer = tf.keras.optimizers.sgd(learning_rate = 0.001)\n",
        "\n",
        "# train the model\n",
        "history = model.fit(x_train, y_train, epochs=50, batch_size=64, validation_data=(x_valid, y_valid), verbose=2)\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range (1, len(loss) + 1)\n",
        "\n",
        "# plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "# plt.plot(epochs, val_loss, 'b', label='validation_loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# one way to test the model\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "# another way to test the model using individual data points (test normalized)\n",
        "prediction = model.predict (x_test)\n",
        "print (prediction)\n",
        "\n",
        "# plt.clf()\n",
        "# plt.title('Test data and predicted values')\n",
        "# plt.scatter (x_test, y_test)\n",
        "# plt.scatter (x_test, prediction)\n",
        "# plt.show ()\n",
        "\n",
        "# convert the model to tflite (float)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "regressionfloat = converter.convert()\n",
        "\n",
        "# save the converted model as a file\n",
        "open(\"regressionfloat\", \"wb\").write(regressionfloat)\n",
        "\n",
        "# export the converted float model to a header file\n",
        "!xxd -i regressionfloat > regressionfloat.h\n",
        "\n",
        "# convert the model to int8 (quantization)\n",
        "\n",
        "# quantize the model to int8 and then convert and export\n",
        "\n",
        "# we need a representative dataset function\n",
        "def representative_dataset (num_samples=500):\n",
        "  for i in range (num_samples):\n",
        "    yield[x_values[i].reshape(1, 1)]\n",
        "\n",
        "# convert the model to tflite (int8)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "converter.representative_dataset = representative_dataset\n",
        "regressionint8 = converter.convert()\n",
        "\n",
        "# save the converted model\n",
        "open(\"regressionint8\", \"wb\").write(regressionint8)\n",
        "\n",
        "# export the converted int8 model to a header file\n",
        "!xxd -i regressionint8 > regressionint8.h\n",
        "\n",
        "## make sure to change the name of the exported header file to model_data.h\n",
        "## inside the header, define the array as: const unsigned char g_model[]\n",
        "## in the bottom of the file, define the length as: unsigned int g_model_len =\n"
      ]
    }
  ]
}